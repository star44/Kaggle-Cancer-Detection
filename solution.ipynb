{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.gitignore',\n",
       " '.ipynb_checkpoints',\n",
       " 'CAMELYON dataset.pdf',\n",
       " 'features.csv',\n",
       " 'histopathologic-cancer-detection',\n",
       " 'histopathologic-cancer-detection.zip',\n",
       " 'solution.ipynb']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split up dataset\n",
    "\n",
    "A key finding in this project has been the size of the dataset that I have to navigate every time I want to create a batch. Since most of the processing is done using a backend written in C, it appears to be quick. However, the time taken to iterate over a list of imaged and read them into arrays may be what is making this network take so long!\n",
    "\n",
    "Therefore, I will be using the Keras `ImageDataGenerator` class! \n",
    "\n",
    "It needs the dataset to be split up by classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shutil import copyfile\n",
    "\n",
    "df = pd.read_csv('histopathologic-cancer-detection/train_labels.csv')\n",
    "cancer = df[df['label'] == 1]\n",
    "noncancer = df[df['label'] == 0]\n",
    "\n",
    "for filename in cancer['id']:\n",
    "    copyfile(\n",
    "        'histopathologic-cancer-detection/train/{0}'.format(filename + '.tif'),\n",
    "        'histopathologic-cancer-detection/train/cancer/{0}'.format(filename + '.tif')\n",
    "    )\n",
    "\n",
    "for filename in noncancer['id']:\n",
    "    copyfile(\n",
    "        'histopathologic-cancer-detection/train/{0}'.format(filename + '.tif'),\n",
    "        'histopathologic-cancer-detection/train/noncancer/{0}'.format(filename + '.tif')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a batch generator\n",
    "\n",
    "As per the keras documentation, the `flow_from_directory` function returns a tuple of (x, y) arrays. This is consistent with the format that can be used in the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 220025 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "image_generator = keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "batch_generator = image_generator.flow_from_directory(\n",
    "    'histopathologic-cancer-detection/split_dataset/',\n",
    "    target_size=(96, 96),\n",
    "    class_mode='binary',\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a model\n",
    "\n",
    "I will attempt to define a model from scratch and train it on my GPU. It seems to perform quite well against the GPU benchmarks written in the keras examples so I have confidence. This message would not exist for long if I was not successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\n9183167\\AppData\\Local\\Continuum\\anaconda3\\envs\\kaggle-competitions\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 155s 155ms/step - loss: 0.4104 - acc: 0.8252\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 151s 151ms/step - loss: 0.2998 - acc: 0.8728\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 151s 151ms/step - loss: 0.2502 - acc: 0.8994\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 152s 152ms/step - loss: 0.2162 - acc: 0.9163\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 150s 150ms/step - loss: 0.1970 - acc: 0.9246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22aecfdde80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import BatchNormalization, Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.backend import var, sum, min, max\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "model = Sequential()\n",
    "# Convolution layer 1\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        64,\n",
    "        kernel_size=(5, 5),\n",
    "        input_shape=(96, 96, 3),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Convolution layer 2\n",
    "model.add(Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # 48\n",
    "\n",
    "# Convolution layer 3\n",
    "model.add(Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # 24\n",
    "\n",
    "# Convolution layer 4\n",
    "model.add(Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # 12\n",
    "\n",
    "# Convolution layer 5\n",
    "model.add(Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # 6\n",
    "\n",
    "# Convolution layer 6\n",
    "model.add(Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # 3\n",
    "\n",
    "\n",
    "# FC\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.binary_crossentropy,\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit_generator(\n",
    "    batch_generator,\n",
    "    steps_per_epoch=1000, \n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
