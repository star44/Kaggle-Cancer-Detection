{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.gitignore',\n",
       " '.ipynb_checkpoints',\n",
       " 'CAMELYON dataset.pdf',\n",
       " 'features.csv',\n",
       " 'histopathologic-cancer-detection',\n",
       " 'histopathologic-cancer-detection.zip',\n",
       " 'solution.ipynb']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split up dataset\n",
    "\n",
    "A key finding in this project has been the size of the dataset that I have to navigate every time I want to create a batch. Since most of the processing is done using a backend written in C, it appears to be quick. However, the time taken to iterate over a list of imaged and read them into arrays may be what is making this network take so long!\n",
    "\n",
    "Therefore, I will be using the Keras `ImageDataGenerator` class! \n",
    "\n",
    "It needs the dataset to be split up by classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shutil import copyfile\n",
    "\n",
    "df = pd.read_csv('histopathologic-cancer-detection/train_labels.csv')\n",
    "cancer = df[df['label'] == 1]\n",
    "noncancer = df[df['label'] == 0]\n",
    "\n",
    "for filename in cancer['id']:\n",
    "    copyfile(\n",
    "        'histopathologic-cancer-detection/train/{0}'.format(filename + '.tif'),\n",
    "        'histopathologic-cancer-detection/train/cancer/{0}'.format(filename + '.tif')\n",
    "    )\n",
    "\n",
    "for filename in noncancer['id']:\n",
    "    copyfile(\n",
    "        'histopathologic-cancer-detection/train/{0}'.format(filename + '.tif'),\n",
    "        'histopathologic-cancer-detection/train/noncancer/{0}'.format(filename + '.tif')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a batch generator\n",
    "\n",
    "As per the keras documentation, the `flow_from_directory` function returns a tuple of (x, y) arrays. This is consistent with the format that can be used in the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 220025 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "image_generator = keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "batch_generator = image_generator.flow_from_directory(\n",
    "    'histopathologic-cancer-detection/split_dataset/',\n",
    "    target_size=(96, 96),\n",
    "    class_mode='binary',\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a model\n",
    "\n",
    "I will attempt to define a model from scratch and train it on my GPU. It seems to perform quite well against the GPU benchmarks written in the keras examples so I have confidence. This message would not exist for long if I was not successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\n9183167\\AppData\\Local\\Continuum\\anaconda3\\envs\\kaggle-competitions\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\n9183167\\AppData\\Local\\Continuum\\anaconda3\\envs\\kaggle-competitions\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\n9183167\\AppData\\Local\\Continuum\\anaconda3\\envs\\kaggle-competitions\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/7\n",
      "1000/1000 [==============================] - 162s 162ms/step - loss: 0.3985 - acc: 0.8293\n",
      "Epoch 2/7\n",
      "1000/1000 [==============================] - 162s 162ms/step - loss: 0.2773 - acc: 0.8862\n",
      "Epoch 3/7\n",
      "1000/1000 [==============================] - 163s 163ms/step - loss: 0.2348 - acc: 0.9080\n",
      "Epoch 4/7\n",
      "1000/1000 [==============================] - 156s 156ms/step - loss: 0.2092 - acc: 0.9188\n",
      "Epoch 5/7\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.1948 - acc: 0.9264\n",
      "Epoch 6/7\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.1803 - acc: 0.9319\n",
      "Epoch 7/7\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.1676 - acc: 0.9372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a97ed4d0f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import BatchNormalization, Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.backend import var, sum, min, max\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "model = Sequential()\n",
    "# Convolution layer 1\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        64,\n",
    "        kernel_size=(5, 5),\n",
    "        input_shape=(96, 96, 3),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Convolution layer 2\n",
    "model.add(Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # 48\n",
    "\n",
    "# Convolution layer 3\n",
    "model.add(Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # 24\n",
    "\n",
    "# Convolution layer 4\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # 12\n",
    "\n",
    "# Convolution layer 5\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # 6\n",
    "\n",
    "\n",
    "\n",
    "# FC\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.binary_crossentropy,\n",
    "    optimizer=keras.optimizers.Adadelta(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit_generator(\n",
    "    batch_generator,\n",
    "    steps_per_epoch=1000, \n",
    "    epochs=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57458/57458 [06:30<00:00, 147.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_files = np.asarray(\n",
    "    os.listdir('histopathologic-cancer-detection/test/')\n",
    ")\n",
    "\n",
    "dataset = []\n",
    "for image_file in tqdm(image_files):\n",
    "    hash_id = image_file.split('.')[0]\n",
    "    prediction = model.predict(\n",
    "        imageio.imread('histopathologic-cancer-detection/test/{0}'.format(image_file)).reshape(1, 96, 96, 3)\n",
    "    )[0][0]\n",
    "    dataset.append([hash_id, prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset, columns=['id', 'label'])\n",
    "df.to_csv('solution.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
